{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPt5q27L5557"
   },
   "source": [
    "# Collaborative Filtering (CS246)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0-YhEpP_Ds-"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zsj5WYpR9QId"
   },
   "source": [
    "Let's setup Spark on your Colab environment.  Run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k-qHai2252mI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/jaeyong/opt/anaconda3/lib/python3.8/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9 in /Users/jaeyong/opt/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install -U -q PyDrive\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUUjUvXe3Sjk"
   },
   "source": [
    "Now we authenticate a Google Drive client to download the filea we will be processing in our Spark job.\n",
    "\n",
    "**Make sure to follow the interactive instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwtlO4_m_LbQ"
   },
   "source": [
    "If you executed the cells above, you should be able to see the dataset we will use for this Colab under the \"Files\" tab on the left panel.\n",
    "\n",
    "Next, we import some of the common libraries needed for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "twk-K-jilWK7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrJlMBt1Ela"
   },
   "source": [
    "Let's initialize the Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vm3sAVeK1EDZ"
   },
   "outputs": [],
   "source": [
    "# create the session\n",
    "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqovskkH1DmC"
   },
   "source": [
    "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DueQggJc1DDk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jaeyongs-mbp:4051\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcf62bea940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iid7lXcG1CY8"
   },
   "source": [
    "If you are running this Colab on the Google hosted runtime, the cell below will create a *ngrok* tunnel which will allow you to still check the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sDnGLVPH1BPQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open ngrok-stable-linux-amd64.zip, ngrok-stable-linux-amd64.zip.zip or ngrok-stable-linux-amd64.zip.ZIP.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jaeyong/opt/anaconda3/lib/python3.8/json/__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "  File \"/Users/jaeyong/opt/anaconda3/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/Users/jaeyong/opt/anaconda3/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/Users/jaeyong/opt/anaconda3/lib/python3.8/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "get_ipython().system_raw('./ngrok http 4050 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAYRX2PMm0L6"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hXdMR6wnEIM"
   },
   "source": [
    "In this Colab, we will be using the [MovieLens dataset](https://grouplens.org/datasets/movielens/), specifically the 100K dataset (which contains in total 100,000 ratings from 1000 users on ~1700 movies).\n",
    "\n",
    "We load the ratings data in a 80%-20% ```training```/```test``` split, while the ```items``` dataframe contains the movie titles associated to the item identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5K93ABEy9Zlo"
   },
   "outputs": [],
   "source": [
    "schema_ratings = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"item_id\", IntegerType(), False),\n",
    "    StructField(\"rating\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", IntegerType(), False)])\n",
    "\n",
    "schema_items = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), False),\n",
    "    StructField(\"movie\", StringType(), False)])\n",
    "\n",
    "training = spark.read.option(\"sep\", \"\\t\").csv(\"./dataset/MovieLens.training\", header=False, schema=schema_ratings)\n",
    "test = spark.read.option(\"sep\", \"\\t\").csv(\"./dataset/MovieLens.test\", header=False, schema=schema_ratings)\n",
    "items = spark.read.option(\"sep\", \"|\").csv(\"./dataset/MovieLens.item\", header=False, schema=schema_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MC_m1oygCoEm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "81Vgo4ovCqtQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- movie: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRaF2A_j_nC7"
   },
   "source": [
    "### Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zM9w2aUvJ7KN"
   },
   "source": [
    "Let's compute some stats!  What is the number of ratings in the training and test dataset? How many movies are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8XZaH16t_CIw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "movie_counts = training.groupBy(\"item_id\")\\\n",
    "                    .agg(count(\"*\").alias(\"MovieCount\"))\\\n",
    "                    .sort(desc(\"MovieCount\"))\n",
    "\n",
    "movie_counts.count()\n",
    "#traing 영화 개수는 1650개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpsaYOqRxar2"
   },
   "source": [
    "Using the training set, train a model with the Alternating Least Squares method available in the Spark MLlib: [https://spark.apache.org/docs/latest/ml-collaborative-filtering.html](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Oitav_xhQD9w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_counts_test = items.groupBy(\"item_id\")\\\n",
    "                    .agg(count(\"*\").alias(\"MovieCount\"))\\\n",
    "                    .sort(desc(\"MovieCount\"))\n",
    "\n",
    "movie_counts_test.count()\n",
    "#test 영화 개수는 1650개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtR1xRvonxiO"
   },
   "source": [
    "Now compute the RMSE on the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GP23Xkgwi0SD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|item_id|      MovieRating|\n",
      "+-------+-----------------+\n",
      "|   1122|              5.0|\n",
      "|   1653|              5.0|\n",
      "|   1201|              5.0|\n",
      "|   1293|              5.0|\n",
      "|   1467|              5.0|\n",
      "|   1599|              5.0|\n",
      "|   1189|              5.0|\n",
      "|   1500|              5.0|\n",
      "|   1449|4.714285714285714|\n",
      "|   1367|4.666666666666667|\n",
      "|    408|4.559139784946237|\n",
      "|    169|4.525773195876289|\n",
      "|    850|              4.5|\n",
      "|   1642|              4.5|\n",
      "|   1594|              4.5|\n",
      "|    318|4.489451476793249|\n",
      "|    483|4.441025641025641|\n",
      "|     64|4.417040358744394|\n",
      "|     12|4.398104265402844|\n",
      "|     50|4.359504132231405|\n",
      "+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_ratings = training.groupBy('item_id')\\\n",
    "                        .agg(mean('rating').alias('MovieRating'))\\\n",
    "                        .sort(desc('MovieRating'))\n",
    "\n",
    "movie_ratings.show()\n",
    "#영화별 평점(평균) 내림차순"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBvSaWGEMHXI"
   },
   "source": [
    "At this point, you can use the trained model to produce the top-K recommendations for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lines = training.rdd\n",
    "ratings = spark.createDataFrame(lines)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1, item_id=1, rating=5, timestamp=874965758),\n",
       " Row(user_id=1, item_id=2, rating=3, timestamp=876893171),\n",
       " Row(user_id=1, item_id=3, rating=4, timestamp=878542960)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.1185899533373342\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=1580, recommendations=[Row(user_id=300, rating=1.5532881021499634), Row(user_id=107, rating=1.5013761520385742), Row(user_id=261, rating=1.462632656097412), Row(user_id=78, rating=1.456456184387207), Row(user_id=39, rating=1.4514678716659546), Row(user_id=626, rating=1.4084563255310059), Row(user_id=97, rating=1.3475079536437988), Row(user_id=366, rating=1.3370845317840576), Row(user_id=362, rating=1.311409592628479), Row(user_id=258, rating=1.2446609735488892)]),\n",
       " Row(item_id=471, recommendations=[Row(user_id=337, rating=6.2447710037231445), Row(user_id=180, rating=6.158797740936279), Row(user_id=636, rating=5.850564956665039), Row(user_id=340, rating=5.534483432769775), Row(user_id=335, rating=5.439809322357178), Row(user_id=173, rating=5.363190650939941), Row(user_id=278, rating=5.182185173034668), Row(user_id=252, rating=5.171838760375977), Row(user_id=372, rating=5.147116661071777), Row(user_id=849, rating=5.111682891845703)]),\n",
       " Row(item_id=1591, recommendations=[Row(user_id=131, rating=11.306530952453613), Row(user_id=209, rating=11.090509414672852), Row(user_id=302, rating=10.217376708984375), Row(user_id=139, rating=10.215211868286133), Row(user_id=281, rating=10.140925407409668), Row(user_id=337, rating=9.960244178771973), Row(user_id=107, rating=9.93110179901123), Row(user_id=765, rating=8.802854537963867), Row(user_id=68, rating=8.657979965209961), Row(user_id=284, rating=8.509005546569824)])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRecs.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIrXJyVNP2AI"
   },
   "source": [
    "Once you have working code for each cell above, **head over to Gradescope, read carefully the questions, and submit your solution for this Colab**!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS246 - Colab 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
